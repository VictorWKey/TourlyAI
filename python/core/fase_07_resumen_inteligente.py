"""
Fase 07: Resumen Inteligente de Rese√±as
========================================
Genera res√∫menes estrat√©gicos seleccionando rese√±as representativas
y usando LLM para crear insights profesionales para turism√≥logos.
"""

import json
import logging
import os
import time
import warnings
from collections import defaultdict
from datetime import datetime

import pandas as pd
from tqdm import tqdm

# Importar proveedor de LLM unificado y utilidades robustas
from .llm_provider import crear_chain, get_llm
from .llm_utils import RetryConfig

warnings.filterwarnings('ignore')

# Configurar logging
logger = logging.getLogger(__name__)


class ResumidorInteligente:
    """
    Genera res√∫menes estrat√©gicos de rese√±as tur√≠sticas usando:
    1. Selecci√≥n inteligente de rese√±as representativas
    2. Categor√≠a dominante basada en probabilidades del modelo
    3. Res√∫menes recursivos por categor√≠a usando LLM
    4. M√∫ltiples formatos de resumen configurables
    """

    def __init__(self, top_n_subtopicos: int = 3, incluir_neutros: bool = False):
        """
        Inicializa el resumidor.

        Args:
            top_n_subtopicos: N√∫mero m√°ximo de subt√≥picos a incluir por categor√≠a.
                             Solo se seleccionan los N subt√≥picos con m√°s rese√±as.
                             Default: 3 (reduce significativamente el uso de tokens)
            incluir_neutros: Si True, incluye rese√±as con sentimiento Neutro.
                            Si False, solo usa Positivo y Negativo (m√°s eficiente).
                            Default: False (recomendado para res√∫menes accionables)
        """
        from config.config import ConfigDataset

        self.dataset_path = str(ConfigDataset.get_dataset_path())
        self.scores_path = str(ConfigDataset.get_shared_dir() / 'categorias_scores.json')
        self.output_path = ConfigDataset.get_shared_dir() / 'resumenes.json'
        self.top_n_subtopicos = top_n_subtopicos
        self.incluir_neutros = incluir_neutros

        self.df = None
        self.scores = None
        self.llm = None

    def _cargar_datos(self):
        """Carga el dataset y las probabilidades de categor√≠as."""
        # Cargar dataset
        if not os.path.exists(self.dataset_path):
            raise FileNotFoundError(f'Dataset no encontrado: {self.dataset_path}')

        self.df = pd.read_csv(self.dataset_path)

        # Verificar columnas requeridas
        columnas_requeridas = ['TituloReview', 'Sentimiento', 'Subjetividad']
        columnas_faltantes = [col for col in columnas_requeridas if col not in self.df.columns]

        if columnas_faltantes:
            raise KeyError(
                f'Columnas requeridas no encontradas: {", ".join(columnas_faltantes)}\n'
                '   Aseg√∫rate de ejecutar las fases previas:\n'
                '   - Fase 01: Procesamiento B√°sico (agrega TituloReview)\n'
                '   - Fase 03: An√°lisis de Sentimientos (agrega Sentimiento)\n'
                '   - Fase 04: An√°lisis de Subjetividad (agrega Subjetividad)'
            )

        # Cargar scores de categor√≠as
        if not os.path.exists(self.scores_path):
            raise FileNotFoundError(
                f'Probabilidades de categor√≠as no encontradas: {self.scores_path}\n'
                'Aseg√∫rate de ejecutar primero la Fase 05 (Clasificaci√≥n de Categor√≠as).'
            )

        with open(self.scores_path, encoding='utf-8') as f:
            self.scores = json.load(f)

        # Verificar si hay scores v√°lidos
        if not self.scores or len(self.scores) == 0:
            print('   ‚ö†Ô∏è  Advertencia: No se encontraron probabilidades de categor√≠as')
            print('      El archivo existe pero est√° vac√≠o. Esto puede ocurrir si:')
            print('      - El modelo de categor√≠as no pudo procesar las rese√±as')
            print('      - Todas las categor√≠as tienen probabilidad 0')

        print(f'   ‚Ä¢ Dataset cargado: {len(self.df)} rese√±as')
        print(f'   ‚Ä¢ Probabilidades cargadas: {len(self.scores)} registros')

        # Advertir si falta la columna Topico (fase 06)
        if 'Topico' not in self.df.columns:
            print("   ‚ö†Ô∏è  Advertencia: Columna 'Topico' no encontrada")
            print('      Los res√∫menes no incluir√°n informaci√≥n de subt√≥picos')
            print('      Ejecuta la Fase 06 (An√°lisis Jer√°rquico de T√≥picos) para mejorar los res√∫menes')

    def _obtener_categoria_dominante(self, idx: int) -> str | None:
        """
        Obtiene la categor√≠a dominante basada en las probabilidades del modelo.

        Args:
            idx: √çndice de la rese√±a

        Returns:
            Nombre de la categor√≠a con mayor probabilidad, o None si no hay
        """
        if str(idx) not in self.scores:
            return None

        categoria_scores = self.scores[str(idx)]

        if not categoria_scores:
            return None

        # Obtener categor√≠a con mayor probabilidad
        categoria_dominante = max(categoria_scores.items(), key=lambda x: x[1])

        return categoria_dominante[0]

    def _obtener_topico_para_categoria(self, idx: int, categoria: str) -> str | None:
        """
        Obtiene el t√≥pico espec√≠fico de una categor√≠a para una rese√±a.

        Args:
            idx: √çndice de la rese√±a
            categoria: Nombre de la categor√≠a

        Returns:
            Nombre del t√≥pico para esa categor√≠a, o None si no hay
        """
        # Verificar si la columna Topico existe
        if 'Topico' not in self.df.columns:
            return None

        topico_str = self.df.loc[idx, 'Topico']

        if pd.isna(topico_str) or topico_str == '{}':
            return None

        try:
            # Parsear el diccionario string
            import ast

            topico_dict = ast.literal_eval(topico_str)
            return topico_dict.get(categoria, None)
        except Exception:
            return None

    # ‚îÄ‚îÄ Configurable thresholds ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # Minimum number of reviews that must survive each filter stage.
    # When a filter would reduce the pool below this threshold the
    # filter is automatically relaxed (see _seleccionar_rese√±as_representativas).
    MIN_RESE√ëAS_POR_ETAPA = 5

    def _seleccionar_rese√±as_representativas(self) -> pd.DataFrame:
        """
        Selects representative reviews using an **adaptive filtering strategy**.

        The method applies a sequence of increasingly selective filters.
        After every filter it checks whether enough reviews remain; if the
        pool falls below ``MIN_RESE√ëAS_POR_ETAPA`` the filter is rolled back
        or relaxed automatically.  This guarantees that the method always
        returns a non-empty result for any dataset size (‚â• 1 review).

        Filtering stages
        ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        1. Subjectivity filter   ‚Äì prefer 'Mixta', fall back to all
        2. Sentiment filter      ‚Äì exclude 'Neutro' (configurable)
        3. Dominant-category     ‚Äì from ``categorias_scores.json``
        4. Relevant topic        ‚Äì from ``Topico`` column (optional)
        5. Top-N subtopic prune  ‚Äì keep only most frequent subtopics
        6. De-duplication        ‚Äì one review per Sentiment √ó Category (√ó Topic)

        Returns
        -------
        pd.DataFrame
            Subset of ``self.df`` enriched with helper columns
            ``CategoriaDominante``, ``TopicoRelevante`` and ``Longitud``.
        """
        total = len(self.df)
        print(f'\n   Seleccionando rese√±as representativas ({total} rese√±as)...')

        # ‚îÄ‚îÄ helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        def _hay_suficientes(df: pd.DataFrame) -> bool:
            return len(df) >= self.MIN_RESE√ëAS_POR_ETAPA

        filtros_aplicados: list[str] = []
        filtros_relajados: list[str] = []

        # ‚îÄ‚îÄ Stage 1: Subjectivity ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        df_filtrado = self.df[self.df['Subjetividad'] == 'Mixta'].copy()

        if _hay_suficientes(df_filtrado):
            filtros_aplicados.append("Subjetividad = 'Mixta'")
        else:
            # Fall back: include 'Subjetiva' too
            df_filtrado = self.df[self.df['Subjetividad'].isin(['Mixta', 'Subjetiva'])].copy()

            if _hay_suficientes(df_filtrado):
                filtros_aplicados.append("Subjetividad ‚àà {'Mixta', 'Subjetiva'}")
                filtros_relajados.append(
                    f'Subjetividad: se incluyeron Subjetivas (solo {self.df["Subjetividad"].eq("Mixta").sum()} Mixtas)'
                )
            else:
                # Fall back: use everything
                df_filtrado = self.df.copy()
                filtros_relajados.append('Subjetividad: sin filtrar (dataset muy peque√±o)')

        # ‚îÄ‚îÄ Stage 2: Sentiment ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        if not self.incluir_neutros:
            df_sin_neutros = df_filtrado[df_filtrado['Sentimiento'].isin(['Positivo', 'Negativo'])]

            if _hay_suficientes(df_sin_neutros):
                eliminadas = len(df_filtrado) - len(df_sin_neutros)
                df_filtrado = df_sin_neutros
                filtros_aplicados.append(f"Sentimiento ‚â† 'Neutro' (‚àí{eliminadas})")
            else:
                filtros_relajados.append(
                    f'Sentimiento: se mantuvieron Neutros ({len(df_filtrado) - len(df_sin_neutros)} neutros preservados)'
                )

        # ‚îÄ‚îÄ Stage 3: Dominant category ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        df_filtrado['CategoriaDominante'] = df_filtrado.index.map(lambda idx: self._obtener_categoria_dominante(idx))

        df_con_categoria = df_filtrado[df_filtrado['CategoriaDominante'].notna()]

        if _hay_suficientes(df_con_categoria):
            df_filtrado = df_con_categoria
            filtros_aplicados.append('Tiene categor√≠a dominante')
        else:
            # Assign a fallback category so the pipeline can continue
            df_filtrado['CategoriaDominante'] = df_filtrado['CategoriaDominante'].fillna('General')
            filtros_relajados.append(
                f"Categor√≠a: {df_filtrado['CategoriaDominante'].eq('General').sum()} rese√±as asignadas a 'General'"
            )

        # ‚îÄ‚îÄ Stage 4: Relevant topic ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        topicos_relevantes = []
        for idx, row in df_filtrado.iterrows():
            topico = self._obtener_topico_para_categoria(idx, row['CategoriaDominante'])
            topicos_relevantes.append(topico if topico else 'General')
        df_filtrado = df_filtrado.copy()
        df_filtrado['TopicoRelevante'] = topicos_relevantes

        # ‚îÄ‚îÄ Stage 5: Top-N subtopic pruning ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        df_top = self._filtrar_top_subtopicos(df_filtrado)

        if _hay_suficientes(df_top):
            df_filtrado = df_top
            filtros_aplicados.append(f'Top {self.top_n_subtopicos} subt√≥picos por categor√≠a')
        else:
            filtros_relajados.append('Subt√≥picos: sin filtro top-N (insuficientes rese√±as)')

        # ‚îÄ‚îÄ Stage 6: Length & date helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        df_filtrado['Longitud'] = df_filtrado['TituloReview'].str.len()

        tiene_fecha = 'FechaEstadia' in df_filtrado.columns
        if tiene_fecha:
            df_filtrado['FechaEstadia'] = pd.to_datetime(df_filtrado['FechaEstadia'], errors='coerce')

        # ‚îÄ‚îÄ Stage 7: De-duplication (one per combination) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        # Build groupby columns adaptively
        columnas_grupo = ['Sentimiento', 'CategoriaDominante']
        tiene_topicos_reales = (df_filtrado['TopicoRelevante'] != 'General').any()
        if tiene_topicos_reales:
            columnas_grupo.append('TopicoRelevante')

        rese√±as_seleccionadas = []
        agrupaciones = df_filtrado.groupby(columnas_grupo, dropna=False)

        for _, grupo in agrupaciones:
            sort_cols = ['Longitud']
            sort_asc = [False]
            if tiene_fecha:
                sort_cols.append('FechaEstadia')
                sort_asc.append(False)

            grupo_ordenado = grupo.sort_values(by=sort_cols, ascending=sort_asc)
            rese√±as_seleccionadas.append(grupo_ordenado.iloc[0])

        df_seleccionado = pd.DataFrame(rese√±as_seleccionadas)

        # ‚îÄ‚îÄ Final safety net ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        # If STILL empty after all relaxations, take the longest reviews
        if len(df_seleccionado) == 0:
            n_fallback = min(10, len(self.df))
            df_seleccionado = self.df.copy()
            df_seleccionado['Longitud'] = df_seleccionado['TituloReview'].str.len()
            df_seleccionado = df_seleccionado.nlargest(n_fallback, 'Longitud')
            df_seleccionado['CategoriaDominante'] = 'General'
            df_seleccionado['TopicoRelevante'] = 'General'
            filtros_relajados.append(f'Fallback: se usaron las {n_fallback} rese√±as m√°s largas sin filtrar')

        # ‚îÄ‚îÄ Report ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        print(f'   ‚úì Rese√±as seleccionadas: {len(df_seleccionado)} de {total}')
        print(f'   ‚úì Reducci√≥n: {total - len(df_seleccionado)} rese√±as filtradas')

        if filtros_aplicados:
            print(f'   ‚Ä¢ Filtros aplicados ({len(filtros_aplicados)}):')
            for f in filtros_aplicados:
                print(f'     ‚úì {f}')

        if filtros_relajados:
            print(f'   ‚Ä¢ Filtros relajados ({len(filtros_relajados)}):')
            for f in filtros_relajados:
                print(f'     ‚ö†Ô∏è  {f}')

        print('   ‚Ä¢ Por categor√≠a:')
        for categoria, count in df_seleccionado['CategoriaDominante'].value_counts().items():
            if 'TopicoRelevante' in df_seleccionado.columns:
                n_sub = df_seleccionado[df_seleccionado['CategoriaDominante'] == categoria]['TopicoRelevante'].nunique()
                print(f'     - {categoria}: {count} rese√±as, {n_sub} subt√≥picos')
            else:
                print(f'     - {categoria}: {count} rese√±as')

        return df_seleccionado

    def _filtrar_top_subtopicos(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Filtra el DataFrame para quedarse solo con los top N subt√≥picos
        m√°s frecuentes de cada categor√≠a.

        Args:
            df: DataFrame con columnas 'CategoriaDominante' y 'TopicoRelevante'

        Returns:
            DataFrame filtrado con solo los subt√≥picos m√°s representativos.
            Returns the original df unchanged if it is empty or has no valid
            categories/topics.
        """
        if len(df) == 0:
            return df

        if 'CategoriaDominante' not in df.columns or 'TopicoRelevante' not in df.columns:
            return df

        dfs_filtrados = []

        for categoria in df['CategoriaDominante'].unique():
            # Filtrar rese√±as de esta categor√≠a
            df_categoria = df[df['CategoriaDominante'] == categoria].copy()

            # Contar frecuencia de subt√≥picos
            conteo_subtopicos = df_categoria['TopicoRelevante'].value_counts()

            # Seleccionar top N subt√≥picos
            top_subtopicos = conteo_subtopicos.head(self.top_n_subtopicos).index.tolist()

            # Filtrar solo esos subt√≥picos
            df_top = df_categoria[df_categoria['TopicoRelevante'].isin(top_subtopicos)]

            dfs_filtrados.append(df_top)

        if not dfs_filtrados:
            return df

        # Concatenar todos los DataFrames filtrados
        df_resultado = pd.concat(dfs_filtrados, ignore_index=False)

        return df_resultado

    def _inicializar_llm(self):
        """Inicializa el modelo LLM para generaci√≥n de res√∫menes."""
        self.llm = get_llm()

    def _generar_resumen_categoria(self, rese√±as: list[dict], categoria: str, tipo_resumen: str) -> str:
        """
        Genera un resumen para una categor√≠a espec√≠fica.

        Args:
            rese√±as: Lista de rese√±as de la categor√≠a
            categoria: Nombre de la categor√≠a
            tipo_resumen: 'estructurado' (only supported type)

        Returns:
            Texto del resumen
        """
        # Preparar contexto de rese√±as
        contexto_rese√±as = ''
        for i, rese√±a in enumerate(rese√±as, 1):
            sentimiento = rese√±a.get('Sentimiento', 'Desconocido')
            topico = rese√±a.get('TopicoRelevante', 'General')
            texto = rese√±a.get('TituloReview', '')[:500]  # Limitar longitud

            contexto_rese√±as += f'\n[Rese√±a {i}] Sentimiento: {sentimiento} | Subt√≥pico: {topico}\n{texto}\n'

        # Template for structured summary
        analysis_language = os.environ.get('ANALYSIS_LANGUAGE', 'es')

        if analysis_language == 'en':
            template = """You are an expert tourism analyst examining tourist opinions.

Category: {categoria}

Representative reviews:
{rese√±as}

Generate a structured summary using RICH Markdown formatting. Follow this template EXACTLY:

## ‚úÖ Positive Aspects
Describe what tourists value about this category. Use a brief introductory sentence, then:
- **[Specific strength]:** Brief explanation
- **[Specific strength]:** Brief explanation
(Include 2-4 bullet points)

## ‚ö†Ô∏è Negative Aspects
Describe main complaints and issues. Use a brief introductory sentence, then:
- **[Specific issue]:** Brief explanation
- **[Specific issue]:** Brief explanation
(Include 2-4 bullet points)

## üè∑Ô∏è Identified Subtopics
List the specific subtopics found as bullet points:
- **[Subtopic name]** ‚Äî one-line description
- **[Subtopic name]** ‚Äî one-line description

IMPORTANT FORMATTING RULES:
- Use ## for section headings (NOT numbered lists)
- Use bullet points (- ) for items
- **Bold** key terms at the start of each bullet
- Leave a blank line between each section
- Maximum 250 words
- Professional, concise tone"""
        else:
            template = """Eres un experto turism√≥logo analizando opiniones de turistas.

Categor√≠a: {categoria}

Rese√±as representativas:
{rese√±as}

Genera un resumen estructurado usando formato Markdown RICO. Sigue esta plantilla EXACTAMENTE:

## ‚úÖ Aspectos Positivos
Describe qu√© valoran los turistas de esta categor√≠a. Usa una oraci√≥n introductoria breve y luego:
- **[Fortaleza espec√≠fica]:** Explicaci√≥n breve
- **[Fortaleza espec√≠fica]:** Explicaci√≥n breve
(Incluye 2-4 vi√±etas)

## ‚ö†Ô∏è Aspectos Negativos
Describe las principales quejas y problemas. Usa una oraci√≥n introductoria breve y luego:
- **[Problema espec√≠fico]:** Explicaci√≥n breve
- **[Problema espec√≠fico]:** Explicaci√≥n breve
(Incluye 2-4 vi√±etas)

## üè∑Ô∏è Subtemas Identificados
Lista los subt√≥picos espec√≠ficos encontrados como vi√±etas:
- **[Nombre del subtema]** ‚Äî descripci√≥n de una l√≠nea
- **[Nombre del subtema]** ‚Äî descripci√≥n de una l√≠nea

REGLAS DE FORMATO IMPORTANTES:
- Usa ## para encabezados de secci√≥n (NO listas numeradas)
- Usa vi√±etas (- ) para los elementos
- **Negritas** para t√©rminos clave al inicio de cada vi√±eta
- Deja una l√≠nea en blanco entre cada secci√≥n
- M√°ximo 250 palabras
- Tono profesional y conciso"""

        # Usar el proveedor de LLM con reintentos
        resumen = self._invocar_llm_con_retry(
            template=template,
            input_data={'categoria': categoria, 'rese√±as': contexto_rese√±as},
            max_retries=3,
            descripcion=f'resumen {tipo_resumen} para {categoria}',
        )

        return resumen.strip() if resumen else f'[No se pudo generar resumen para {categoria}]'

    def _invocar_llm_con_retry(
        self, template: str, input_data: dict, max_retries: int = 3, descripcion: str = 'operaci√≥n LLM'
    ) -> str:
        """
        Invoca el LLM con reintentos y manejo de errores.

        Args:
            template: Template del prompt
            input_data: Datos de entrada
            max_retries: N√∫mero de reintentos
            descripcion: Descripci√≥n para logs

        Returns:
            Respuesta del LLM o string vac√≠o
        """
        from .llm_utils import is_openai_quota_error

        config = RetryConfig(max_retries=max_retries)
        ultimo_error = None

        for intento in range(max_retries + 1):
            try:
                chain = crear_chain(template)
                resultado = chain.invoke(input_data)

                if resultado and str(resultado).strip():
                    return str(resultado)

                raise ValueError('Respuesta vac√≠a del LLM')

            except Exception as e:
                # Don't retry non-transient errors (quota/billing)
                if is_openai_quota_error(e):
                    logger.error(
                        f'OpenAI quota exhausted for {descripcion}. '
                        'Add funds at https://platform.openai.com/account/billing'
                    )
                    raise RuntimeError(
                        'OPENAI_QUOTA_EXHAUSTED: Tu API key de OpenAI no tiene cr√©ditos disponibles. '
                        'Agrega fondos en https://platform.openai.com/account/billing '
                        'o cambia al modo de IA local (Ollama) en la configuraci√≥n.'
                    ) from e

                ultimo_error = e
                logger.warning(f'Intento {intento + 1}/{max_retries + 1} fall√≥ para {descripcion}: {str(e)[:100]}')

                if intento < max_retries:
                    pass  # retry immediately

        logger.error(f'Todos los reintentos fallaron para {descripcion}: {ultimo_error}')
        return ''

    def _generar_resumen_global(self, resumenes_por_categoria: dict[str, str], tipo_resumen: str) -> str:
        """
        Genera un resumen global combinando los res√∫menes por categor√≠a.

        Args:
            resumenes_por_categoria: Diccionario {categoria: resumen}
            tipo_resumen: 'estructurado' (only supported type)

        Returns:
            Texto del resumen global
        """
        # Preparar contexto
        contexto = ''
        for categoria, resumen in resumenes_por_categoria.items():
            contexto += f'\n**{categoria}**:\n{resumen}\n'

        analysis_language = os.environ.get('ANALYSIS_LANGUAGE', 'es')

        if analysis_language == 'en':
            template = """You are an expert tourism analyst synthesizing tourist opinions.

Summaries by category:
{resumenes}

Generate a structured executive summary (300-400 words) using RICH Markdown formatting. Follow this template:

## üåç General Overview
Provide a global panorama of tourism perception. Synthesize the overall sentiment and experience in 2-3 sentences.

---

## üí™ Destination Strengths
Highlight the best valued categories:
- **[Category name]:** What makes it outstanding (1 sentence)
- **[Category name]:** What makes it outstanding (1 sentence)
- **[Category name]:** What makes it outstanding (1 sentence)

---

## üìã Opportunity Areas
Categories that need attention and improvement:
- **[Category name]:** Key issue to address (1 sentence)
- **[Category name]:** Key issue to address (1 sentence)

---

## ‚≠ê Highlighted Aspects
Important specific mentions that stand out across categories:
- **[Aspect]:** Brief description
- **[Aspect]:** Brief description

IMPORTANT FORMATTING RULES:
- Use ## for each major section heading
- Include --- (horizontal rule) between sections for visual separation
- Use bullet points with **bold lead-ins**
- Leave blank lines between sections
- Professional executive-report tone"""
        else:
            template = """Eres un experto turism√≥logo sintetizando opiniones tur√≠sticas.

Res√∫menes por categor√≠a:
{resumenes}

Genera un resumen ejecutivo estructurado (300-400 palabras) usando formato Markdown RICO. Sigue esta plantilla:

## üåç Resumen General
Proporciona un panorama global de la percepci√≥n tur√≠stica. Sintetiza el sentimiento general y la experiencia en 2-3 oraciones.

---

## üí™ Fortalezas del Destino
Destaca las categor√≠as mejor valoradas:
- **[Nombre de categor√≠a]:** Qu√© la hace sobresaliente (1 oraci√≥n)
- **[Nombre de categor√≠a]:** Qu√© la hace sobresaliente (1 oraci√≥n)
- **[Nombre de categor√≠a]:** Qu√© la hace sobresaliente (1 oraci√≥n)

---

## üìã √Åreas de Oportunidad
Categor√≠as que necesitan atenci√≥n y mejora:
- **[Nombre de categor√≠a]:** Problema clave a atender (1 oraci√≥n)
- **[Nombre de categor√≠a]:** Problema clave a atender (1 oraci√≥n)

---

## ‚≠ê Aspectos Destacados
Menciones espec√≠ficas importantes que sobresalen entre categor√≠as:
- **[Aspecto]:** Descripci√≥n breve
- **[Aspecto]:** Descripci√≥n breve

REGLAS DE FORMATO IMPORTANTES:
- Usa ## para cada encabezado de secci√≥n principal
- Incluye --- (l√≠nea horizontal) entre secciones para separaci√≥n visual
- Usa vi√±etas con **negritas al inicio**
- Deja l√≠neas en blanco entre secciones
- Tono profesional de reporte ejecutivo"""

        # Usar el proveedor de LLM con reintentos
        resumen_global = self._invocar_llm_con_retry(
            template=template,
            input_data={'resumenes': contexto},
            max_retries=3,
            descripcion=f'resumen global {tipo_resumen}',
        )

        return resumen_global.strip() if resumen_global else '[No se pudo generar resumen global]'

    def _generar_resumenes(self, df_seleccionado: pd.DataFrame, tipos_resumen: list[str] | None = None) -> dict:
        """
        Genera res√∫menes recursivos por categor√≠a y globales.
        Only generates structured summaries.

        Args:
            df_seleccionado: DataFrame con rese√±as seleccionadas
            tipos_resumen: Deprecated, ignored. Always generates ['estructurado']

        Returns:
            Diccionario con todos los res√∫menes generados
        """
        # Only structured summaries are supported
        tipos_resumen = ['estructurado']
        print('\n   Generando res√∫menes con LLM...')

        # Inicializar LLM
        self._inicializar_llm()

        resultado = {
            'metadata': {
                'fecha_generacion': datetime.now().isoformat(),
                'total_rese√±as_dataset': len(self.df),
                'rese√±as_seleccionadas': len(df_seleccionado),
                'tipos_resumen': tipos_resumen,
                'top_subtopicos_por_categoria': self.top_n_subtopicos,
                'incluir_neutros': self.incluir_neutros,
                'sentimientos_incluidos': ['Positivo', 'Neutro', 'Negativo']
                if self.incluir_neutros
                else ['Positivo', 'Negativo'],
                'reduccion_porcentaje': round((1 - len(df_seleccionado) / len(self.df)) * 100, 2),
            },
            'resumenes': {},
        }

        # Agrupar rese√±as por categor√≠a dominante
        rese√±as_por_categoria = defaultdict(list)

        for _, row in df_seleccionado.iterrows():
            categoria = row['CategoriaDominante']
            rese√±as_por_categoria[categoria].append(row.to_dict())

        # Calcular total de tareas para la barra de progreso
        total_tareas = len(tipos_resumen) * (len(rese√±as_por_categoria) + 1)  # +1 por resumen global

        # Generar res√∫menes para cada tipo solicitado con una sola barra de progreso
        with tqdm(total=total_tareas, desc='   Progreso') as pbar:
            for tipo in tipos_resumen:
                print(f'   ‚Ä¢ Generando resumen tipo: {tipo}')

                resultado['resumenes'][tipo] = {'por_categoria': {}, 'global': None}

                # Res√∫menes por categor√≠a
                resumenes_categoria = {}
                for categoria, rese√±as in rese√±as_por_categoria.items():
                    resumen = self._generar_resumen_categoria(rese√±as, categoria, tipo)

                    resumenes_categoria[categoria] = resumen
                    resultado['resumenes'][tipo]['por_categoria'][categoria] = resumen
                    pbar.update(1)

                # Resumen global
                resumen_global = self._generar_resumen_global(resumenes_categoria, tipo)
                resultado['resumenes'][tipo]['global'] = resumen_global
                pbar.update(1)

        return resultado

    def _guardar_resultado(self, resultado: dict):
        """
        Guarda el resultado en JSON.

        Args:
            resultado: Diccionario con los res√∫menes generados
        """
        # Crear carpeta shared si no existe
        os.makedirs(os.path.dirname(self.output_path), exist_ok=True)

        with open(self.output_path, 'w', encoding='utf-8') as f:
            json.dump(resultado, f, ensure_ascii=False, indent=2)

        print(f'\n   ‚úì Res√∫menes guardados en: {self.output_path}')

    def ya_procesado(self):
        """
        Verifica si esta fase ya fue ejecutada.
        Revisa si existe el archivo de res√∫menes.
        """
        return self.output_path.exists()

    def procesar(self, tipos_resumen: list[str] | None = None, forzar: bool = False):
        """
        Ejecuta el pipeline completo de generaci√≥n de res√∫menes.
        Only generates structured summaries.

        Args:
            tipos_resumen: Deprecated, ignored. Always generates structured summary.
            forzar: Si es True, ejecuta incluso si ya fue procesado
        """
        if not forzar and self.ya_procesado():
            print('   ‚è≠Ô∏è  Fase ya ejecutada previamente (omitiendo)')
            return

        # Only structured summaries are generated
        tipos_resumen = ['estructurado']

        print('Generating structured summary...')

        # 1. Cargar datos
        self._cargar_datos()

        # 2. Seleccionar rese√±as representativas
        # The adaptive strategy guarantees a non-empty result for any dataset ‚â• 1 review
        df_seleccionado = self._seleccionar_rese√±as_representativas()

        if len(df_seleccionado) == 0:
            print('‚ö†Ô∏è  No se encontraron rese√±as representativas. Verifica el dataset.')
            return

        # 3. Generar res√∫menes
        resultado = self._generar_resumenes(df_seleccionado, tipos_resumen)

        # 4. Guardar resultado
        self._guardar_resultado(resultado)

        print('\n‚úÖ Res√∫menes generados exitosamente')
        print(f'   ‚Ä¢ Categor√≠as resumidas: {len(resultado["resumenes"][tipos_resumen[0]]["por_categoria"])}')
        print(f'   ‚Ä¢ Tipos de resumen: {len(tipos_resumen)}')
